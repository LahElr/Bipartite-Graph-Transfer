{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dgl\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from networkx.algorithms import bipartite\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(os.path.join(os.getcwd(), '../clean_data/human'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileNxData(object):\n",
    "    \"\"\"In tis BiGraph, we think (src_nodes == genes) and (tgt_nodes == cells)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, species, tissue, number, data_path):\n",
    "        # put species, tissue, number into one tuple for easy param passing\n",
    "        self.target_file_tuple = (species, tissue, str(number))\n",
    "        self.G = nx.DiGraph()\n",
    "        \n",
    "        # collect location of *_data.csv and *_celltype.csv\n",
    "        cell_type_file, data_file = self.get_file_path(data_path)\n",
    "        \n",
    "        # add gene nodes\n",
    "        src_list = self.get_src(data_file)\n",
    "        self.add_src_nodes(src_list, cell_type_file, data_file)\n",
    "        \n",
    "        # add cell nodes (cell has type attribute called 'type_name')\n",
    "        tgt_list = self.get_tgt(cell_type_file)\n",
    "        self.add_tgt_nodes(tgt_list, cell_type_file, data_file)\n",
    "        \n",
    "        # add edges between gene and cell with weight in form of numpy array (weight > 0)\n",
    "        weight_mat = self.get_weight(data_file)\n",
    "        self.add_edges_with_weight(src_list, tgt_list, weight_mat)\n",
    "        \n",
    "        self.G = self.delete_isolated_nodes()\n",
    "\n",
    "    def get_file_path(self, data_path):\n",
    "        species, tissue, number = self.target_file_tuple\n",
    "        celltype_file_name = f'{species}_{tissue}{number}_celltype.csv'\n",
    "        data_file_name = f'{species}_{tissue}{number}_data.csv'\n",
    "        celltype_file = data_path / celltype_file_name\n",
    "        data_file = data_path / data_file_name\n",
    "        return celltype_file, data_file\n",
    "\n",
    "    def get_tgt(self, cell_type_file):\n",
    "        species, tissue, number = self.target_file_tuple\n",
    "        cell_name_df = pd.read_csv(cell_type_file, dtype=str, usecols=[0])\n",
    "        cell_name_list = cell_name_df.values[:,0].tolist()\n",
    "        cell_name_list = [species + '_' + tissue + '_' + number + '_' + cell_name for cell_name in cell_name_list]\n",
    "        return cell_name_list\n",
    "\n",
    "    def get_src(self, data_file):\n",
    "        gene_name_df = pd.read_csv(data_file, dtype=str, usecols=[0])\n",
    "        gene_name_list = gene_name_df.values[:,0].tolist()\n",
    "        return gene_name_list\n",
    "\n",
    "    def get_weight(self, data_file):\n",
    "        gene_cell_mat_df = pd.read_csv(data_file)\n",
    "        gene_cell_mat = gene_cell_mat_df.values[:,1:]\n",
    "        return gene_cell_mat\n",
    "\n",
    "    def add_src_nodes(self, src_list, cell_type_file, data_file):\n",
    "        self.G.add_nodes_from(src_list, bipartite=0)\n",
    "    \n",
    "    def add_tgt_nodes(self, tgt_list, cell_type_file, data_file):\n",
    "        self.G.add_nodes_from(tgt_list, bipartite=1)\n",
    "        # add cell_type attribute\n",
    "        species, tissue, number = self.target_file_tuple\n",
    "        cell_df = pd.read_csv(cell_type_file, usecols=[0,1])\n",
    "        # change Cell | Cell_type two columsn of csv into {Cell: Cell_type} dict\n",
    "        cell_dict = cell_df.set_index('Cell').T.to_dict('records')[0]\n",
    "        # add 'type_name' attributes based on 'Cell_type' in the dict\n",
    "        for key, value in cell_dict.items():\n",
    "            self.G.nodes[species + '_' + tissue + '_' + number + '_' + key]['type_name'] = value\n",
    "    \n",
    "    def add_edges_with_weight(self,src_list, tgt_list, weight_mat):\n",
    "        for i in range(len(src_list)):\n",
    "            for j in range(len(tgt_list)):\n",
    "                # if weight == 0, we ignore this edge\n",
    "                if (weight_mat[i][j] > 0):\n",
    "                    self.G.add_edge(src_list[i], tgt_list[j], weight=weight_mat[i][j])\n",
    "                    # if bidirected, we need to add this line, else not\n",
    "                    #self.G.add_edge(tgt_list[j], src_list[i], weight=weight_mat[i][j])\n",
    "\n",
    "    def delete_isolated_nodes(self):\n",
    "        self.G.remove_nodes_from(list(nx.isolates(self.G)))\n",
    "        return self.G\n",
    "    \n",
    "    def print_nx_graph(self):\n",
    "        # Separate by group\n",
    "        l, r = nx.bipartite.sets(self.G)\n",
    "        pos = {}\n",
    "        # Update position for node from each group\n",
    "        pos.update((node, (1, index)) for index, node in enumerate(l))\n",
    "        pos.update((node, (2, index)) for index, node in enumerate(r))\n",
    "        nx.draw(self.G, with_labels=True, pos=pos)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeciesNxData(object):\n",
    "    def __init__(self, filebigraph_list):\n",
    "        graph_list = [graph.G for graph in filebigraph_list]\n",
    "        self.G = nx.compose_all(graph_list)\n",
    "        self.node_index_dict = self.save_node_index_dict()\n",
    "        self.type_name_dict = self.save_type_name_dict()\n",
    "        self.gene_name_dict, self.cell_name_dict = self.save_gene_cell_name_dict()\n",
    "        self.gene_num, self.cell_num = self.check_gene_cell_num()\n",
    "        self.get_gene_embed()\n",
    "        self.get_cell_embed()\n",
    "        self.dgl_G = self.nx_to_dgl()\n",
    "    \n",
    "    def save_node_index_dict(self):\n",
    "        # node_index_dict : {\"C_1\": 1, \"10001\": 2}\n",
    "        gene_set = {n for n, d in self.G.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "        cell_set = set(self.G) - gene_set\n",
    "        nx_node_name_list = list(gene_set) + list(cell_set)\n",
    "        \n",
    "        node_index_dict = {}\n",
    "        for i in range(len(nx_node_name_list)):\n",
    "            node_index_dict[nx_node_name_list[i]] = i\n",
    "        return node_index_dict\n",
    "    \n",
    "    def save_type_name_dict(self):\n",
    "        # type_name_dict : {\"C_1\" : \"T Cell\"}\n",
    "        type_name_dict = nx.get_node_attributes(self.G, \"type_name\")\n",
    "        return type_name_dict\n",
    "         \n",
    "    def save_gene_cell_name_dict(self):\n",
    "        # seperately store gene_name dict and cell_name dict\n",
    "        # two dict : {\"C_1\": 0}, {\"10001\": 1}\n",
    "        gene_set = {n for n, d in self.G.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "        cell_set = set(self.G) - gene_set\n",
    "        gene_name_dict = {gene_name: idx for idx, gene_name in enumerate(list(gene_set))}\n",
    "        cell_name_dict = {cell_name: idx for idx, cell_name in enumerate(list(cell_set))}\n",
    "        return gene_name_dict, cell_name_dict\n",
    "    \n",
    "    def check_gene_cell_num(self):\n",
    "        gene_num = len(self.gene_name_dict)\n",
    "        cell_num = len(self.cell_name_dict)\n",
    "        return gene_num, cell_num\n",
    "    \n",
    "    def get_gene_embed(self):\n",
    "        for n in self.gene_name_dict:\n",
    "            gene_embed = torch.zeros(self.gene_num)\n",
    "            gene_embed[self.gene_name_dict[n]] = 1\n",
    "            self.G.nodes[n]['embed'] = gene_embed\n",
    "\n",
    "    def get_cell_embed(self):\n",
    "        cell_embed = {}\n",
    "        for n in self.cell_name_dict:\n",
    "            cell_embed = torch.zeros(self.gene_num)\n",
    "            for m in self.gene_name_dict:\n",
    "                #if self.G.get_edge_data(m, n, default=0):\n",
    "                #    cell_embed[self.gene_name_dict[m]] = 1\n",
    "                if self.G.get_edge_data(m, n, default=0) and self.G[m][n]['weight'] > 0:\n",
    "                    cell_embed[self.gene_name_dict[m]] = 1\n",
    "            self.G.nodes[n]['embed'] = cell_embed\n",
    "    \n",
    "    def nx_to_dgl(self):\n",
    "        dgl_G = dgl.from_networkx(self.G, edge_attrs=['weight'], node_attrs=['embed'])\n",
    "        dgl_G = self.make_bidirected(dgl_G)\n",
    "        return dgl_G\n",
    "    \n",
    "    def make_bidirected(self, dgl_G):\n",
    "        src_node_tensor = dgl_G.edges()[0]\n",
    "        tgt_node_tensor = dgl_G.edges()[1]\n",
    "        weight_tensor = dgl_G.edata['weight']\n",
    "        dgl_G.add_edges(tgt_node_tensor, src_node_tensor)\n",
    "        return dgl_G\n",
    "    \n",
    "    def print_embed(self):\n",
    "        print(self.gene_name_dict)\n",
    "        for n, _ in self.G.nodes(data=True):\n",
    "            print(n, self.G.nodes[n]['embed'])\n",
    "    \n",
    "    def print_nx_graph(self):\n",
    "        # Separate by group\n",
    "        l, r = nx.bipartite.sets(self.G)\n",
    "        pos = {}\n",
    "        # Update position for node from each group\n",
    "        pos.update((node, (1, index)) for index, node in enumerate(l))\n",
    "        pos.update((node, (2, index)) for index, node in enumerate(r))\n",
    "        nx.draw(self.G, with_labels=True, pos=pos)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time1 : 267.43579149246216\n",
      "time2 : 159.8538544178009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('--------------------dgl_G---------------------')\\nprint(dgl_G)\\nprint(dgl_G.nodes())\\nprint(dgl_G.edges())\\nprint(dgl_G.edata['weight'])\\nprint(dgl_G.ndata['embed'])\\nprint('--------------------dgl_G---------------------')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start1 = time.time()\n",
    "graph1 = FileNxData('human', 'lung', '8426', data_path)\n",
    "end1 = time.time()\n",
    "print('time1 : {}'.format(end1-start1))\n",
    "\n",
    "#graph1.print_nx_graph()\n",
    "start2 = time.time()\n",
    "graph2 = FileNxData('human', 'lung', '6022', data_path)\n",
    "end2 = time.time()\n",
    "print('time2 : {}'.format(end2-start2))\n",
    "#graph2.print_nx_graph()\n",
    "#start3 = time.time()\n",
    "#graph3 = SpeciesNxData([graph1, graph2])\n",
    "#end3 = time.time()\n",
    "#print('time3 : {}'.format(end3-start3))\n",
    "#graph3.print_nx_graph()\n",
    "#dgl_G = graph3.nx_to_dgl()\n",
    "'''\n",
    "print('--------------------dgl_G---------------------')\n",
    "print(dgl_G)\n",
    "print(dgl_G.nodes())\n",
    "print(dgl_G.edges())\n",
    "print(dgl_G.edata['weight'])\n",
    "print(dgl_G.ndata['embed'])\n",
    "print('--------------------dgl_G---------------------')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeciesDGLData(object):\n",
    "    def __init__(self, nx_data):\n",
    "        self.G = nx_data.dgl_G\n",
    "        self.node_index_dict = nx_data.node_index_dict\n",
    "        self.type_name_dict = nx_data.type_name_dict\n",
    "        self.type_index_dict = self.name_to_index(self.type_name_dict)\n",
    "        self.gene_name_dict, self.cell_name_dict = nx_data.gene_name_dict, nx_data.cell_name_dict\n",
    "        self.gene_num, self.cell_num = nx_data.gene_num, nx_data.cell_num\n",
    "        \n",
    "        self.feature = self.extract_feature()\n",
    "        self.label = self.extract_label()\n",
    "        self.train_index, self.valid_index, self.test_index = self.split_cell_nodes()\n",
    "        \n",
    "    def name_to_index(self, type_name_dict):\n",
    "        type_index_dict = {}\n",
    "        type_set = set()\n",
    "        for key, value in type_name_dict.items():\n",
    "            type_set |= {value}\n",
    "        type_list = list(type_set)\n",
    "        for key, value in type_name_dict.items():\n",
    "            type_index_dict[key] = type_list.index(value)\n",
    "        return type_index_dict\n",
    "    \n",
    "    def extract_feature(self):\n",
    "        feature_tensor = self.G.ndata['embed']\n",
    "        return feature_tensor\n",
    "        \n",
    "    def extract_label(self):\n",
    "        label_list = []\n",
    "        for key, _ in self.node_index_dict.items():\n",
    "            if key in self.type_index_dict.keys():\n",
    "                label_list += [self.type_index_dict[key]]\n",
    "            else:\n",
    "                label_list += [-1]\n",
    "        label_tensor = torch.tensor(label_list)\n",
    "        return label_tensor\n",
    "        \n",
    "    def split_cell_nodes(self):\n",
    "        # only calculate loss on cells\n",
    "        # TODO: here we suppose gene name is smaller than cell name\n",
    "        cell_dataset = self.G.nodes()[self.gene_num:]\n",
    "        train_subset, valid_subset, test_subset = dgl.data.utils.split_dataset(cell_dataset, \n",
    "                                                                               shuffle=True, \n",
    "                                                                               frac_list=[0.7, 0.2, 0.1])\n",
    "        train_index = train_subset[:]\n",
    "        valid_index = valid_subset[:]\n",
    "        test_index = test_subset[:]\n",
    "        return train_index, valid_index, test_index\n",
    "    \n",
    "    def load_data(self):\n",
    "        return self.G, self.feature, self.label, self.train_index, self.valid_index, self.test_index\n",
    "    \n",
    "    def print_dgl_graph(self):\n",
    "        fig, ax = plt.subplots()\n",
    "        nx.draw(self.G.to_networkx(), ax=ax)\n",
    "        plt.show()\n",
    "        \n",
    "    def save_graph(self):\n",
    "        dgl.data.utils.save_graphs(\"./dgl_data.bin\", self.G)\n",
    "        \n",
    "    def load_graph(self):\n",
    "        glist, _ = dgl.data.utils.load_graphs(\"./data.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8133392333984375e-05\n",
      "tensor([18491, 22958, 25156,  ..., 17796, 19552, 20332])\n",
      "tensor([25303, 22335, 16544,  ..., 26906, 16759, 27449])\n",
      "tensor([27698, 26187, 24146,  ..., 22476, 21708, 19343])\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nx_data = SpeciesNxData([graph1, graph2])\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "#print(nx_data.type_name_dict)\n",
    "dgl_data = SpeciesDGLData(nx_data)\n",
    "#print(dgl_data.G.edges())\n",
    "#dgl_data.G.edata['weight']\n",
    "#print(dgl_data.test_mask)\n",
    "#print(dgl_data.train_mask)\n",
    "#print(dgl_data.G.nodes())\n",
    "#print(dgl_data.node_index_dict)\n",
    "#print(dgl_data.G.srcdata)\n",
    "#print(dgl_data.G.dstdata)\n",
    "#dgl_data.extract_label()\n",
    "#dgl_data.print_dgl_graph()\n",
    "print(dgl_data.train_index)\n",
    "print(dgl_data.test_index)\n",
    "print(dgl_data.valid_index)\n",
    "#dgl_data.save_graph()\n",
    "#sg = dgl.sampling.select_topk(g=dgl_data.G, k=2, weight='weight', nodes=dgl_data.G.nodes())\n",
    "#fig, ax = plt.subplots()\n",
    "#nx.draw(sg.to_networkx(), ax=ax)\n",
    "#plt.show()\n",
    "#dgl_data.G.nodes()[:dgl_data.gene_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open('nx_data.pkl', 'wb')\n",
    "pickle.dump(nx_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('dgl_data.pkl', 'wb')\n",
    "pickle.dump(dgl_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.065536022186279\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "file1 = open('dgl_data.pkl', 'rb')\n",
    "file2 = open('nx_data.pkl', 'rb')\n",
    "dgl_data = pickle.load(file1)\n",
    "#nx_data = pickle.load(file2)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30587"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn.pytorch.conv import SAGEConv\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        # block is the bipartite graph we sample. Here it is used for message passing.\n",
    "        # x is node feature\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h_dst = h[:block.number_of_dst_nodes()]\n",
    "            h = layer(block, (h, h_dst))\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, x, batch_size, device):\n",
    "        # inference 用于评估测试，针对的是完全图\n",
    "        # 目前会出现重复计算的问题，优化方案还在 to do list 上\n",
    "        nodes = torch.arange(g.number_of_nodes())\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = torch.zeros(g.number_of_nodes(), \n",
    "                         self.n_hidden if l != len(self.layers) - 1 else self.n_classes)\n",
    "            for start in tqdm.trange(0, len(nodes), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_nodes = nodes[start:end]\n",
    "                block = dgl.to_block(dgl.in_subgraph(g, batch_nodes), batch_nodes)\n",
    "                input_nodes = block.srcdata[dgl.NID]\n",
    "                h = x[input_nodes].to(device)\n",
    "                h_dst = h[:block.number_of_dst_nodes()]\n",
    "                h = layer(block, (h, h_dst))\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = self.activation(h)\n",
    "                    h = self.dropout(h)\n",
    "                y[start:end] = h.cpu()\n",
    "            x = y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, fanouts):\n",
    "        \"\"\"\n",
    "        g 为 DGLGraph；\n",
    "        fanouts 为采样节点的数量，实验使用 10,25，指一阶邻居采样 10 个，二阶邻居采样 25 个。\n",
    "        \"\"\"\n",
    "        self.g = g\n",
    "        self.fanouts = fanouts\n",
    "\n",
    "    def sample_blocks(self, seeds):\n",
    "        seeds = torch.LongTensor(np.asarray(seeds))\n",
    "        blocks = []\n",
    "        for fanout in self.fanouts: \n",
    "            # sample_neighbors 可以对每一个种子的节点进行邻居采样并返回相应的子图\n",
    "            # replace=True 表示用采样后的邻居节点代替所有邻居节点\n",
    "            frontier = dgl.sampling.sample_neighbors(self.g, seeds, fanout, replace=True)\n",
    "            # 将图转变为可以用于消息传递的二部图（源节点和目的节点）\n",
    "            # 其中源节点的 id 也可能包含目的节点的 id（原因上面说了）\n",
    "            # 转变为二部图主要是为了方便进行消息传递\n",
    "            block = dgl.to_block(frontier, seeds)\n",
    "            # 获取新图的源节点作为种子节点，为下一层作准备\n",
    "            # 之所以是从 src 中获取种子节点，是因为采样操作相对于聚合操作来说是一个逆向操作\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "            # 把这一层放在最前面。\n",
    "            # PS：如果数据量大的话，插入操作是不是不太友好。\n",
    "            blocks.insert(0, block)\n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class Runner(object):\n",
    "    def __init__(self, dgl_data):\n",
    "        # init and load data from dgl_data object\n",
    "        self.g, self.features, self.labels, self.train_index, self.valid_index, self.test_index = dgl_data.load_data()        \n",
    "        self.batch_size = 512\n",
    "        self.epochs = 300\n",
    "        self.device = 'cpu'\n",
    "        \n",
    "        # init the model\n",
    "        feat_size = self.features.shape[1]\n",
    "        n_hidden = 512\n",
    "        n_classes = torch.max(self.labels).item()+1\n",
    "        n_layers = 1\n",
    "        activation = F.relu\n",
    "        dropout = 0.5\n",
    "        self.model = GraphSAGE(feat_size, n_hidden, n_classes, n_layers, activation, dropout).to(self.device)\n",
    "    \n",
    "        # init the optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "\n",
    "        # init sampler and dataloader to produce batch shape data\n",
    "        fan_out = '5'\n",
    "        num_workers = 4\n",
    "        # Create sampler\n",
    "        self.sampler = NeighborSampler(self.g, [int(fanout) for fanout in fan_out.split(',')])\n",
    "\n",
    "        # Create PyTorch DataLoader for constructing blocks\n",
    "        self.dataloader = DataLoader(\n",
    "            dataset=self.train_index.numpy(),\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.sampler.sample_blocks,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=num_workers)\n",
    "    \n",
    "    def compute_loss(self, logits, labels):\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        loss = F.nll_loss(logp, labels)\n",
    "        return loss\n",
    "    \n",
    "    def compute_acc(self, pred, labels):\n",
    "        return (torch.argmax(pred, dim=1) == labels).float().sum() / len(pred)\n",
    " \n",
    "    '''\n",
    "    def evaluate(self, model, g, inputs, labels, val_index, batch_size, device):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model.inference(g, inputs, batch_size, device)\n",
    "        model.train()\n",
    "        return self.compute_acc(pred[val_index], labels[val_index])\n",
    "    '''\n",
    "    \n",
    "    def evaluate(self, model, g, features, labels, index):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dataloader = DataLoader(\n",
    "                dataset=self.valid_index.numpy(),\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.sampler.sample_blocks,\n",
    "                shuffle=True,\n",
    "                drop_last=False,\n",
    "                num_workers=4)\n",
    "            \n",
    "            for step, blocks in enumerate(dataloader):\n",
    "                tic_step = time.time()\n",
    "                \n",
    "                input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "                seeds = blocks[-1].dstdata[dgl.NID]\n",
    "\n",
    "                # Load the input features as well as output labels\n",
    "                batch_inputs, batch_labels = self.load_subtensor(g, labels, seeds, input_nodes, self.device)\n",
    "\n",
    "                # Compute loss and prediction\n",
    "                batch_preds = self.model(blocks, batch_inputs)\n",
    "                \n",
    "                acc = self.compute_acc(batch_pred, batch_labels)\n",
    "\n",
    "            return acc\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        # Training loop\n",
    "        avg = 0\n",
    "        iter_output = []\n",
    "        for epoch in range(self.epochs):\n",
    "            tic = time.time()\n",
    "\n",
    "            for step, blocks in enumerate(self.dataloader):\n",
    "                tic_step = time.time()\n",
    "                \n",
    "                input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "                seeds = blocks[-1].dstdata[dgl.NID]\n",
    "\n",
    "                # Load the input features as well as output labels\n",
    "                batch_inputs, batch_labels = self.load_subtensor(self.g, self.labels, seeds, input_nodes, self.device)\n",
    "\n",
    "                # Compute loss and prediction\n",
    "                batch_pred = self.model(blocks, batch_inputs)\n",
    "                loss = self.compute_loss(batch_pred, batch_labels)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                iter_output.append(len(seeds) / (time.time() - tic_step))\n",
    "                if step % 2 == 0:\n",
    "                    acc = self.compute_acc(batch_pred, batch_labels)\n",
    "                    gpu_mem_alloc = torch.cuda.max_memory_allocated() / 1000000 if torch.cuda.is_available() else 0\n",
    "                    print('Epoch {:05d} | Step {:05d} | Loss {:.4f} | Train Acc {:.4f} | Speed (samples/sec) {:.4f} | GPU {:.1f} MiB'.format(\n",
    "                        epoch, step, loss.item(), acc.item(), np.mean(iter_output[3:]), gpu_mem_alloc))\n",
    "\n",
    "            toc = time.time()\n",
    "            #print('Epoch Time(s): {:.4f}'.format(toc - tic))\n",
    "            if epoch >= 5:\n",
    "                avg += toc - tic\n",
    "            if  epoch != 0:\n",
    "                print(self.valid_index)\n",
    "                eval_acc = self.evaluate(self.model, self.g, self.features, self.labels, self.valid_index)\n",
    "                print('Eval Acc {:.4f}'.format(eval_acc))\n",
    "\n",
    "        print('Avg epoch time: {}'.format(avg / (epoch - 4)))\n",
    "\n",
    "\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "        # model forward\n",
    "        logits = self.model(self.g, self.features)\n",
    "        # loss calculate\n",
    "        labels = self.labels\n",
    "        loss = self.calculate_loss(logits, labels)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        losses.append(loss)\n",
    "        return np.mean(losses)\n",
    "    \n",
    "    def load_subtensor(self, g, labels, seeds, input_nodes, device):\n",
    "        \"\"\"\n",
    "        将一组节点的特征和标签复制到 GPU 上。\n",
    "        \"\"\"\n",
    "        batch_inputs = self.features[input_nodes].to(device)\n",
    "        batch_labels = labels[seeds].to(device)\n",
    "        return batch_inputs, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Step 00000 | Loss 6.3098 | Train Acc 0.0020 | Speed (samples/sec) nan | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00002 | Loss 6.2631 | Train Acc 0.0000 | Speed (samples/sec) nan | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00004 | Loss 6.1315 | Train Acc 0.0137 | Speed (samples/sec) 270.5808 | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00006 | Loss 6.1212 | Train Acc 0.0215 | Speed (samples/sec) 263.9268 | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00008 | Loss 5.9569 | Train Acc 0.0586 | Speed (samples/sec) 269.4014 | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00010 | Loss 5.9528 | Train Acc 0.0742 | Speed (samples/sec) 265.3685 | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00012 | Loss 5.8687 | Train Acc 0.0840 | Speed (samples/sec) 285.2210 | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00014 | Loss 5.8247 | Train Acc 0.1270 | Speed (samples/sec) 300.4771 | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00016 | Loss 5.7148 | Train Acc 0.1641 | Speed (samples/sec) 303.6623 | GPU 0.0 MiB\n",
      "Epoch 00000 | Step 00018 | Loss 5.6911 | Train Acc 0.1719 | Speed (samples/sec) 303.6466 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00000 | Loss 5.5033 | Train Acc 0.2227 | Speed (samples/sec) 296.1337 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00002 | Loss 5.5177 | Train Acc 0.2012 | Speed (samples/sec) 294.5867 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00004 | Loss 5.4785 | Train Acc 0.2012 | Speed (samples/sec) 294.0416 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00006 | Loss 5.4382 | Train Acc 0.1855 | Speed (samples/sec) 292.8739 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00008 | Loss 5.3092 | Train Acc 0.2070 | Speed (samples/sec) 293.5212 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00010 | Loss 5.2800 | Train Acc 0.2070 | Speed (samples/sec) 290.7646 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00012 | Loss 5.1452 | Train Acc 0.2207 | Speed (samples/sec) 290.9030 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00014 | Loss 5.1775 | Train Acc 0.2188 | Speed (samples/sec) 289.6273 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00016 | Loss 5.0968 | Train Acc 0.2305 | Speed (samples/sec) 287.7188 | GPU 0.0 MiB\n",
      "Epoch 00001 | Step 00018 | Loss 5.0198 | Train Acc 0.2383 | Speed (samples/sec) 286.4437 | GPU 0.0 MiB\n",
      "tensor([27698, 26187, 24146,  ..., 22476, 21708, 19343])\n",
      "torch.Size([329, 512])\n",
      "torch.Size([30587])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (329) must match the size of tensor b (2889) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-ee6015458719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdgl_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-37de4b4b9695>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m           \u001b[0;32mif\u001b[0m  \u001b[0mepoch\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m               \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eval Acc {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-37de4b4b9695>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, g, features, labels, index)\u001b[0m\n\u001b[1;32m     78\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m           \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (329) must match the size of tensor b (2889) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "runner = Runner(dgl_data)\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn.pytorch.conv import SAGEConv\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "    \n",
    "    def forward(self, g, feat):\n",
    "        # Creating a local scope so that all the stored ndata and edata\n",
    "        # (such as the `'h'` ndata below) are automatically popped out\n",
    "        # when the scope exits.\n",
    "        print(feat)\n",
    "        with g.local_scope():\n",
    "            g.srcdata['h1'] = feat\n",
    "            g.update_all(self.edge_message, self.node_reduce)\n",
    "            h = g.ndata['h2']\n",
    "            return self.linear(h)\n",
    "    \n",
    "    @staticmethod\n",
    "    def node_reduce(nodes):\n",
    "        # nodes.data['h'] is a tensor of shape (N, 1),\n",
    "        # nodes.mailbox['m'] is a tensor of shape (N, D, 1),\n",
    "        # where N is the number of nodes in the batch,\n",
    "        # D is the number of messages received per node for this node batch\n",
    "        return {'h2': nodes.dstdata['h1'] + nodes.mailbox['m'].mean(1)}\n",
    "    \n",
    "    @staticmethod\n",
    "    def edge_message(edges):\n",
    "        # edges.src with the shape of (#edges, embed_dim)\n",
    "        # edges.data['weight'] with the shape of (#edges, 1)\n",
    "        # broadcast in order to update with weight\n",
    "        w = edges.data['weight'].float().reshape(-1, 1)\n",
    "        return {'m': edges.src['h1'] * w}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        # TODO: modify the layers in the GCN layer\n",
    "        # input and output for the GCN should be modified to real numbers\n",
    "        self.layer1 = GCNLayer(8, 512)\n",
    "        self.layer2 = GCNLayer(512, 6)\n",
    "\n",
    "    def forward(self, blocks, input_nodes, output_nodes):\n",
    "        print(blocks[0].ndata['embed'])\n",
    "        print(input_nodes)\n",
    "        print(output_nodes)\n",
    "        for block in blocks:\n",
    "            x = F.relu(self.layer1(block, block.ndata['embed']))\n",
    "            print(x)\n",
    "            x = self.layer2(block, x)\n",
    "        return x\n",
    "    \n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type))\n",
    "        # output layer\n",
    "        self.layers.append(SAGEConv(n_hidden, n_classes, aggregator_type)) # activation None\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        h = self.dropout(inputs)\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            h = layer(graph, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "    \n",
    "class Runner(object):\n",
    "    def __init__(self, dgl_data):\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        self.g, self.features, self.labels, self.train_index, self.valid_index, self.test_index = dgl_data.load_data()\n",
    "        self.epochs = 300\n",
    "        \n",
    "        feat_size = self.features.shape[1]\n",
    "        n_hidden = 32\n",
    "        n_classes = torch.max(labels).item()+1\n",
    "        n_layers = 3\n",
    "        activation = F.relu\n",
    "        dropout = 0.5\n",
    "        aggregator_type = 'gcn'\n",
    "        \n",
    "        self.model = GraphSAGE(feat_size, n_hidden, n_classes, n_layers, activation, dropout, aggregator_type)\n",
    "    \n",
    "    def compute_loss(self, logits, labels):\n",
    "        print(logits.shape)\n",
    "        print(labels.shape)\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        loss = F.nll_loss(logp, labels)\n",
    "        return loss\n",
    "    \n",
    "    def compute_acc(pred, labels):\n",
    "        return (torch.argmax(pred, dim=1) == labels).float().sum() / len(pred)\n",
    "    \n",
    "    def evaluate(self, model, g, features, labels, index):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(g, features)\n",
    "            logits = logits[index]\n",
    "            labels = labels[index]\n",
    "            _, indices = torch.max(logits, dim=1)\n",
    "            correct = torch.sum(indices == labels)\n",
    "            return correct.item() * 1.0 / len(labels)\n",
    "    \n",
    "    def train(self):\n",
    "        # Training loop\n",
    "        avg = 0\n",
    "        iter_output = []\n",
    "        for epoch in range(self.epochs):\n",
    "            tic = time.time()\n",
    "\n",
    "            for step, blocks in enumerate(self.dataloader):\n",
    "                tic_step = time.time()\n",
    "\n",
    "                input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "                seeds = blocks[-1].dstdata[dgl.NID]\n",
    "\n",
    "                # Load the input features as well as output labels\n",
    "                batch_inputs, batch_labels = self.load_subtensor(g, labels, seeds, input_nodes, device)\n",
    "\n",
    "                # Compute loss and prediction\n",
    "                batch_pred = self.model(blocks, batch_inputs)\n",
    "                loss = self.compute_loss(batch_pred, batch_labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                iter_tput.append(len(seeds) / (time.time() - tic_step))\n",
    "                if step % log_every == 0:\n",
    "                    acc = self.compute_acc(batch_pred, batch_labels)\n",
    "                    gpu_mem_alloc = th.cuda.max_memory_allocated() / 1000000 if th.cuda.is_available() else 0\n",
    "                    print('Epoch {:05d} | Step {:05d} | Loss {:.4f} | Train Acc {:.4f} | Speed (samples/sec) {:.4f} | GPU {:.1f} MiB'.format(\n",
    "                        epoch, step, loss.item(), acc.item(), np.mean(iter_tput[3:]), gpu_mem_alloc))\n",
    "\n",
    "            toc = time.time()\n",
    "            print('Epoch Time(s): {:.4f}'.format(toc - tic))\n",
    "            if epoch >= 5:\n",
    "                avg += toc - tic\n",
    "            if epoch % eval_every == 0 and epoch != 0:\n",
    "                eval_acc = evaluate(self.model, self.g, self.g.ndata['features'], self.labels, val_mask, batch_size, device)\n",
    "                print('Eval Acc {:.4f}'.format(eval_acc))\n",
    "\n",
    "        print('Avg epoch time: {}'.format(avg / (epoch - 4)))\n",
    "\n",
    "\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "        # model forward\n",
    "        logits = self.model(self.g, self.features)\n",
    "        # loss calculate\n",
    "        labels = self.labels\n",
    "        loss = self.calculate_loss(logits, labels)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        losses.append(loss)\n",
    "        return np.mean(losses)\n",
    "    \n",
    "    def load_subtensor(self, g, labels, seeds, input_nodes, device):\n",
    "        \"\"\"\n",
    "        将一组节点的特征和标签复制到 GPU 上。\n",
    "        \"\"\"\n",
    "        batch_inputs = g.ndata['features'][input_nodes].to(device)\n",
    "        batch_labels = labels[seeds].to(device)\n",
    "        return batch_inputs, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(dgl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[index]\n",
    "        labels = labels[index]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dur = []\n",
    "for epoch in range(300):\n",
    "    for input_nodes, output_nodes, blocks in dataloader:\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        logits = model(blocks)\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        loss = F.nll_loss(logp[input_nodes], labels[output_nodes])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "        acc = evaluate(gcn, g, features, labels, train_index)\n",
    "        print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "                epoch, loss.item(), acc, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([1,1,1])\n",
    "dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, g.nodes()[train_index], sampler,\n",
    "    batch_size=1, shuffle=True, drop_last=False, num_workers=4)\n",
    "for input_nodes, output_nodes, blocks in dataloader:\n",
    "    print(input_nodes)\n",
    "    print(output_nodes)\n",
    "    print(blocks)\n",
    "    print(blocks[0].dstdata['_ID'], blocks[0].srcdata['_ID'])\n",
    "    print(blocks[1].dstdata['_ID'], blocks[1].srcdata['_ID'])\n",
    "    print(blocks[2].dstdata['_ID'], blocks[2].srcdata['_ID'])\n",
    "    print(blocks[0].ndata['embed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphSAGE 的代码实现\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        # block 是我们采样获得的二部图，这里用于消息传播\n",
    "        # x 为节点特征\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h_dst = h[:block.number_of_dst_nodes()]\n",
    "            h = layer(block, (h, h_dst))\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, x, batch_size, device):\n",
    "        # inference 用于评估测试，针对的是完全图\n",
    "        # 目前会出现重复计算的问题，优化方案还在 to do list 上\n",
    "        nodes = th.arange(g.number_of_nodes())\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = th.zeros(g.number_of_nodes(), \n",
    "                         self.n_hidden if l != len(self.layers) - 1 else self.n_classes)\n",
    "            for start in tqdm.trange(0, len(nodes), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_nodes = nodes[start:end]\n",
    "                block = dgl.to_block(dgl.in_subgraph(g, batch_nodes), batch_nodes)\n",
    "                input_nodes = block.srcdata[dgl.NID]\n",
    "                h = x[input_nodes].to(device)\n",
    "                h_dst = h[:block.number_of_dst_nodes()]\n",
    "                h = layer(block, (h, h_dst))\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = self.activation(h)\n",
    "                    h = self.dropout(h)\n",
    "                y[start:end] = h.cpu()\n",
    "            x = y\n",
    "        return y\n",
    "\n",
    "def compute_acc(pred, labels):\n",
    "    \"\"\"\n",
    "    计算准确率\n",
    "    \"\"\"\n",
    "    return (th.argmax(pred, dim=1) == labels).float().sum() / len(pred)\n",
    "\n",
    "def evaluate(model, g, inputs, labels, val_mask, batch_size, device):\n",
    "    \"\"\"\n",
    "    评估模型，调用 model 的 inference 函数\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        pred = model.inference(g, inputs, batch_size, device)\n",
    "    model.train()\n",
    "    return compute_acc(pred[val_mask], labels[val_mask])\n",
    "\n",
    "def load_subtensor(g, labels, seeds, input_nodes, device):\n",
    "    \"\"\"\n",
    "    将一组节点的特征和标签复制到 GPU 上。\n",
    "    \"\"\"\n",
    "    batch_inputs = g.ndata['features'][input_nodes].to(device)\n",
    "    batch_labels = labels[seeds].to(device)\n",
    "    return batch_inputs, batch_labels\n",
    "\n",
    "# 参数设置\n",
    "gpu = -1\n",
    "num_epochs = 20\n",
    "num_hidden = 16\n",
    "num_layers = 2\n",
    "fan_out = '10,25'\n",
    "batch_size = 1000\n",
    "log_every = 20  # 记录日志的频率\n",
    "eval_every = 5\n",
    "lr = 0.003\n",
    "dropout = 0.5\n",
    "num_workers = 0  # 用于采样进程的数量\n",
    "\n",
    "if gpu >= 0:\n",
    "    device = th.device('cuda:%d' % gpu)\n",
    "else:\n",
    "    device = th.device('cpu')\n",
    "\n",
    "# load reddit data\n",
    "# NumNodes: 232965\n",
    "# NumEdges: 114848857\n",
    "# NumFeats: 602\n",
    "# NumClasses: 41\n",
    "# NumTrainingSamples: 153431\n",
    "# NumValidationSamples: 23831\n",
    "# NumTestSamples: 55703\n",
    "data = RedditDataset(self_loop=True)\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "features = th.Tensor(data.features)\n",
    "in_feats = features.shape[1]\n",
    "labels = th.LongTensor(data.labels)\n",
    "n_classes = data.num_labels\n",
    "# Construct graph\n",
    "g = dgl.graph(data.graph.all_edges())\n",
    "g.ndata['features'] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nid = th.LongTensor(np.nonzero(train_mask)[0])\n",
    "val_nid = th.LongTensor(np.nonzero(val_mask)[0])\n",
    "train_mask = th.BoolTensor(train_mask)\n",
    "val_mask = th.BoolTensor(val_mask)\n",
    "\n",
    "# Create sampler\n",
    "sampler = NeighborSampler(g, [int(fanout) for fanout in fan_out.split(',')])\n",
    "\n",
    "# Create PyTorch DataLoader for constructing blocks\n",
    "# collate_fn 参数指定了 sampler，可以对 batch 中的节点进行采样\n",
    "dataloader = DataLoader(\n",
    "    dataset=train_nid.numpy(),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=sampler.sample_blocks,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Define model and optimizer\n",
    "model = GraphSAGE(in_feats, num_hidden, n_classes, num_layers, F.relu, dropout)\n",
    "model = model.to(device)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "loss_fcn = loss_fcn.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "avg = 0\n",
    "iter_tput = []\n",
    "for epoch in range(num_epochs):\n",
    "    tic = time.time()\n",
    "\n",
    "    for step, blocks in enumerate(dataloader):\n",
    "        tic_step = time.time()\n",
    "\n",
    "        input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "        seeds = blocks[-1].dstdata[dgl.NID]\n",
    "\n",
    "        # Load the input features as well as output labels\n",
    "        batch_inputs, batch_labels = load_subtensor(g, labels, seeds, input_nodes, device)\n",
    "\n",
    "        # Compute loss and prediction\n",
    "        batch_pred = model(blocks, batch_inputs)\n",
    "        loss = loss_fcn(batch_pred, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_tput.append(len(seeds) / (time.time() - tic_step))\n",
    "        if step % log_every == 0:\n",
    "            acc = compute_acc(batch_pred, batch_labels)\n",
    "            gpu_mem_alloc = th.cuda.max_memory_allocated() / 1000000 if th.cuda.is_available() else 0\n",
    "            print('Epoch {:05d} | Step {:05d} | Loss {:.4f} | Train Acc {:.4f} | Speed (samples/sec) {:.4f} | GPU {:.1f} MiB'.format(\n",
    "                epoch, step, loss.item(), acc.item(), np.mean(iter_tput[3:]), gpu_mem_alloc))\n",
    "\n",
    "    toc = time.time()\n",
    "    print('Epoch Time(s): {:.4f}'.format(toc - tic))\n",
    "    if epoch >= 5:\n",
    "        avg += toc - tic\n",
    "    if epoch % eval_every == 0 and epoch != 0:\n",
    "        eval_acc = evaluate(model, g, g.ndata['features'], labels, val_mask, batch_size, device)\n",
    "        print('Eval Acc {:.4f}'.format(eval_acc))\n",
    "\n",
    "print('Avg epoch time: {}'.format(avg / (epoch - 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python37064bitbaseconda70b66ab4790b4536b9d484e76e49516a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
